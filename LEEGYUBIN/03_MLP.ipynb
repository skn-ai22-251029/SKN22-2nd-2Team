{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5fd48b8",
   "metadata": {},
   "source": [
    "# KKBox Churn Prediction — Embedding + MLP (Hyperparameter Summary)\n",
    "\n",
    "## 1) Reproducibility / Split\n",
    "- `RANDOM_STATE`: **719**\n",
    "- Split: **Train / Valid / Test = 0.70 / 0.15 / 0.15**\n",
    "  - Step1: `train_test_split(test_size=0.30, stratify=y)`\n",
    "  - Step2: `train_test_split(test_size=0.50, stratify=y_tmp)`  → valid/test 15%/15%\n",
    "\n",
    "## 2) Feature Set\n",
    "- Target: `TARGET_COL = \"is_churn\"` (binary, 0/1)\n",
    "- ID: `ID_COL = \"msno\"`\n",
    "- Features: `FEATURE_COLS = CATEGORICAL_COLS + NUMERICAL_COLS`\n",
    "  - Categorical (Embedding input): `CATEGORICAL_COLS = [...]`\n",
    "  - Numerical (Scaler input): `NUMERICAL_COLS = [...]`\n",
    "- 실험(e0/e1/e2/e3/e3.1)은 위 리스트에서 **컬럼 주석 ON/OFF**로 통제\n",
    "\n",
    "## 3) Preprocess\n",
    "### Numerical\n",
    "- Missing value: **train median impute**\n",
    "- Scaling: **StandardScaler** (fit on train, transform on valid/test)\n",
    "\n",
    "### Categorical\n",
    "- Encoding: **train-vocab 기반 index mapping**\n",
    "  - UNK/NaN → **0**\n",
    "  - seen category → **1..V**\n",
    "- Input dtype: `object` 강제(카테고리 에러 방지)\n",
    "\n",
    "## 4) Model Architecture (Embedding + MLP)\n",
    "- Output: **1 logit** → `sigmoid(logit)` = `P(is_churn=1)`\n",
    "- Hidden sizes: `HIDDEN = (256, 128, 64)`\n",
    "- Dropout: `DROPOUT = 0.35`\n",
    "- Normalization: **BatchNorm1d(in_dim)** (concat 입력에 1회 적용)\n",
    "- Activation: **ReLU**\n",
    "\n",
    "### Embedding dimension rule (per categorical column)\n",
    "- `d_i = min(50, max(2, round(sqrt(V_i) * 2)))`\n",
    "  - `V_i`: 해당 컬럼의 train 카테고리 개수(UNK 제외)\n",
    "\n",
    "## 5) Optimization / Loss (Imbalance Handling)\n",
    "- Loss: **BCEWithLogitsLoss**\n",
    "- Class imbalance weight:\n",
    "  - `pos_weight = n_neg / n_pos`  (train에서 자동 계산)\n",
    "- Optimizer: **AdamW**\n",
    "  - Learning rate: `LR = 2e-3`\n",
    "  - Weight decay: `WEIGHT_DECAY = 1e-4`\n",
    "\n",
    "## 6) Training\n",
    "- Batch size: `BATCH_SIZE = 4096`\n",
    "- Max epochs: `MAX_EPOCHS = 30`\n",
    "- Early stopping:\n",
    "  - Metric: **Valid PR-AUC**\n",
    "  - Patience: `PATIENCE = 5`\n",
    "\n",
    "## 7) Evaluation\n",
    "- Core metrics (Valid/Test 공통):\n",
    "  - **PR-AUC**, **ROC-AUC**, **LogLoss**, **Accuracy@0.5**\n",
    "- Threshold-based report:\n",
    "  - `threshold = 0.5`\n",
    "  - Confusion Matrix / Classification Report @ 0.5\n",
    "\n",
    "## 8) (Optional) Permutation Importance (PR-AUC drop)\n",
    "- `RUN_PERM_IMPORTANCE`: True/False\n",
    "- `PERM_TOP_N`: 30\n",
    "- `PERM_MAX_SAMPLES`: 200000\n",
    "- Importance definition:\n",
    "  - drop = (baseline PR-AUC) − (PR-AUC after shuffling one feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1cc1a0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 0) Imports & Seed\n",
    "# ============================================================\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, log_loss,\n",
    "    accuracy_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "RANDOM_STATE = 719\n",
    "\n",
    "def seed_everything(seed=RANDOM_STATE):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743a707b",
   "metadata": {},
   "source": [
    "## 1) Config (한 곳에서만 수정)\n",
    "\n",
    "- 아래에서 **컬럼 ON/OFF(주석)** 으로 feature set을 관리하세요.\n",
    "- `DATA_PATH`만 본인 환경에 맞게 조정하면 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29c04e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATEGORICAL_COLS: 6\n",
      "NUMERICAL_COLS  : 44\n",
      "FEATURE_COLS    : 50\n",
      "BATCH_SIZE      : 4096\n",
      "MAX_EPOCHS      : 30 PATIENCE: 5\n",
      "LR/WEIGHT_DECAY : 0.002 0.0001\n",
      "HIDDEN/DROPOUT  : (256, 128, 64) 0.35\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1) Config\n",
    "# ============================================================\n",
    "DATA_PATH = \"kkbox_train_feature_v3.parquet\"   # 필요 시 수정\n",
    "\n",
    "RANDOM_STATE = 719\n",
    "\n",
    "ID_COL = \"msno\"\n",
    "TARGET_COL = \"is_churn\"\n",
    "\n",
    "CATEGORICAL_COLS = [\n",
    "    \"city\", \"gender\", \"registered_via\", \"last_payment_method\",\n",
    "    \"has_ever_paid\", \"has_ever_cancelled\",\n",
    "    # \"is_auto_renew_last\",\n",
    "    # \"is_free_user\",\n",
    "]\n",
    "\n",
    "NUMERICAL_COLS = [\n",
    "    \"reg_days\",\n",
    "\n",
    "    # ======================\n",
    "    # w7\n",
    "    # ======================\n",
    "    \"num_days_active_w7\", \"total_secs_w7\", \"avg_secs_per_day_w7\", \"std_secs_w7\",\n",
    "    \"num_songs_w7\", \"avg_songs_per_day_w7\", \"num_unq_w7\", \"num_25_w7\", \"num_100_w7\",\n",
    "    \"short_play_w7\", \"skip_ratio_w7\", \"completion_ratio_w7\", \"short_play_ratio_w7\", \"variety_ratio_w7\",\n",
    "\n",
    "    # ======================\n",
    "    # w14\n",
    "    # ======================\n",
    "    \"num_days_active_w14\", \"total_secs_w14\", \"avg_secs_per_day_w14\", \"std_secs_w14\",\n",
    "    \"num_songs_w14\", \"avg_songs_per_day_w14\", \"num_unq_w14\", \"num_25_w14\", \"num_100_w14\",\n",
    "    \"short_play_w14\", \"skip_ratio_w14\", \"completion_ratio_w14\", \"short_play_ratio_w14\", \"variety_ratio_w14\",\n",
    "\n",
    "    # ======================\n",
    "    # w21\n",
    "    # ======================\n",
    "    \"num_days_active_w21\", \"total_secs_w21\", \"avg_secs_per_day_w21\", \"std_secs_w21\",\n",
    "    \"num_songs_w21\", \"avg_songs_per_day_w21\", \"num_unq_w21\", \"num_25_w21\", \"num_100_w21\",\n",
    "    \"short_play_w21\", \"skip_ratio_w21\", \"completion_ratio_w21\", \"short_play_ratio_w21\", \"variety_ratio_w21\",\n",
    "\n",
    "    # ======================\n",
    "    # w30  (OFF → 주석 처리)\n",
    "    # ======================\n",
    "    # \"num_days_active_w30\", \"total_secs_w30\", \"avg_secs_per_day_w30\", \"std_secs_w30\",\n",
    "    # \"num_songs_w30\", \"avg_songs_per_day_w30\", \"num_unq_w30\", \"num_25_w30\", \"num_100_w30\",\n",
    "    # \"short_play_w30\", \"skip_ratio_w30\", \"completion_ratio_w30\", \"short_play_ratio_w30\", \"variety_ratio_w30\",\n",
    "\n",
    "    # ======================\n",
    "    # trend (주의: 상위 window에 종속됨)\n",
    "    # ======================\n",
    "    # w7–w14\n",
    "    \"days_trend_w7_w14\",\n",
    "\n",
    "    # w7–w30 / w14–w30 (w30 OFF 시 같이 OFF)\n",
    "    # \"secs_trend_w7_w30\", \"secs_trend_w14_w30\",\n",
    "    # \"days_trend_w7_w30\",\n",
    "    # \"songs_trend_w7_w30\", \"songs_trend_w14_w30\",\n",
    "    # \"skip_trend_w7_w30\", \"completion_trend_w7_w30\",\n",
    "\n",
    "    # ======================\n",
    "    # transactions (logs-only 실험 시 OFF)\n",
    "    # ======================\n",
    "    # \"days_since_last_payment\", \"days_since_last_cancel\", \"last_plan_days\",\n",
    "    # \"total_payment_count\", \"total_amount_paid\", \"avg_amount_per_payment\",\n",
    "    # \"unique_plan_count\", \"subscription_months_est\",\n",
    "    # \"payment_count_last_30d\", \"payment_count_last_90d\",\n",
    "]\n",
    "\n",
    "FEATURE_COLS = CATEGORICAL_COLS + NUMERICAL_COLS\n",
    "\n",
    "# ------------------------------\n",
    "# Training Hyperparams\n",
    "# ------------------------------\n",
    "BATCH_SIZE = 4096\n",
    "MAX_EPOCHS = 30\n",
    "PATIENCE = 5\n",
    "LR = 2e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "HIDDEN = (256, 128, 64)\n",
    "DROPOUT = 0.35\n",
    "\n",
    "# ------------------------------\n",
    "# Optional: Permutation Importance\n",
    "# ------------------------------\n",
    "RUN_PERM_IMPORTANCE = True\n",
    "PERM_TOP_N = 30          # 상위 N개만 출력\n",
    "PERM_MAX_SAMPLES = 200000 # 너무 오래 걸리면 줄이세요 (예: 50000)\n",
    "\n",
    "def print_config():\n",
    "    print(\"CATEGORICAL_COLS:\", len(CATEGORICAL_COLS))\n",
    "    print(\"NUMERICAL_COLS  :\", len(NUMERICAL_COLS))\n",
    "    print(\"FEATURE_COLS    :\", len(FEATURE_COLS))\n",
    "    print(\"BATCH_SIZE      :\", BATCH_SIZE)\n",
    "    print(\"MAX_EPOCHS      :\", MAX_EPOCHS, \"PATIENCE:\", PATIENCE)\n",
    "    print(\"LR/WEIGHT_DECAY :\", LR, WEIGHT_DECAY)\n",
    "    print(\"HIDDEN/DROPOUT  :\", HIDDEN, DROPOUT)\n",
    "\n",
    "print_config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141aed4f",
   "metadata": {},
   "source": [
    "## 2) Load & Validate → X, y 생성\n",
    "\n",
    "- 지정 컬럼이 누락되면 즉시 에러로 중단합니다(실험 공정성/재현성).\n",
    "- `X = df[FEATURE_COLS]`, `y = df[TARGET_COL]` 방식은 요청하신 그대로 유지합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2cec70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DATA_PATH: kkbox_train_feature_v3.parquet\n",
      "df shape: (860966, 85)\n",
      "X shape : (860966, 50)\n",
      "y shape : (860966,) pos rate: 0.09460071594000227\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2) Load & Validate\n",
    "# ============================================================\n",
    "# (로컬 실행/샌드박스 실행 모두 대비)\n",
    "candidate_paths = [DATA_PATH, f\"/mnt/data/{os.path.basename(DATA_PATH)}\", \"/mnt/data/kkbox_train_feature_v3.parquet\"]\n",
    "for p in candidate_paths:\n",
    "    if os.path.exists(p):\n",
    "        DATA_PATH = p\n",
    "        break\n",
    "\n",
    "print(\"Using DATA_PATH:\", DATA_PATH)\n",
    "\n",
    "df = pd.read_parquet(DATA_PATH)\n",
    "\n",
    "required_cols = [ID_COL, TARGET_COL] + FEATURE_COLS\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"[ERROR] Missing columns ({len(missing)}): {missing}\")\n",
    "\n",
    "X = df[FEATURE_COLS].copy()\n",
    "y = df[TARGET_COL].astype(int).copy()\n",
    "ids = df[ID_COL].copy()\n",
    "\n",
    "print(\"df shape:\", df.shape)\n",
    "print(\"X shape :\", X.shape)\n",
    "print(\"y shape :\", y.shape, \"pos rate:\", float(y.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d1ca49",
   "metadata": {},
   "source": [
    "## 3) Fixed Split (Train/Valid/Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c3f83bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split sizes: 602676 129145 129145\n",
      "split ratios: 0.6999997677027897 0.15000011614860517 0.15000011614860517\n",
      "pos rate | train/valid/test: 0.09460141104009451 0.09459909404158116 0.09459909404158116\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3) Fixed Split (70/15/15) - no file save\n",
    "# ============================================================\n",
    "\n",
    "idx_all = np.arange(len(df))\n",
    "\n",
    "# 1) Train 70%, Temp 30%\n",
    "tr_idx, tmp_idx = train_test_split(\n",
    "    idx_all,\n",
    "    test_size=0.30,\n",
    "    stratify=y.values,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 2) Temp 30%를 Valid 15%, Test 15%로 50:50 분할\n",
    "va_idx, te_idx = train_test_split(\n",
    "    tmp_idx,\n",
    "    test_size=0.50,\n",
    "    stratify=y.values[tmp_idx],\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "X_tr, y_tr = X.iloc[tr_idx].copy(), y.iloc[tr_idx].copy()\n",
    "X_va, y_va = X.iloc[va_idx].copy(), y.iloc[va_idx].copy()\n",
    "X_te, y_te = X.iloc[te_idx].copy(), y.iloc[te_idx].copy()\n",
    "\n",
    "print(\"split sizes:\", len(tr_idx), len(va_idx), len(te_idx))\n",
    "print(\"split ratios:\", len(tr_idx)/len(df), len(va_idx)/len(df), len(te_idx)/len(df))\n",
    "print(\"pos rate | train/valid/test:\",\n",
    "      float(y_tr.mean()), float(y_va.mean()), float(y_te.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfd0a9b",
   "metadata": {},
   "source": [
    "## 4) Preprocess\n",
    "\n",
    "- Numerical: median impute + StandardScaler\n",
    "- Categorical: train-vocab mapping → index (unseen/NaN = 0)\n",
    "  - `pandas.Categorical` 관련 에러 방지를 위해 `astype(\"object\")`를 강제합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db90e04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric shape: (602676, 44)\n",
      "Cat shape    : (602676, 6)\n",
      "cat_sizes    : {'city': 22, 'gender': 4, 'registered_via': 6, 'last_payment_method': 34, 'has_ever_paid': 3, 'has_ever_cancelled': 3}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4) Preprocess\n",
    "# ============================================================\n",
    "# ---- Numeric: median + scaler (train 기준)\n",
    "num_median = X_tr[NUMERICAL_COLS].median(numeric_only=True)\n",
    "\n",
    "X_tr_num = X_tr[NUMERICAL_COLS].fillna(num_median)\n",
    "X_va_num = X_va[NUMERICAL_COLS].fillna(num_median)\n",
    "X_te_num = X_te[NUMERICAL_COLS].fillna(num_median)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr_num = scaler.fit_transform(X_tr_num).astype(np.float32)\n",
    "X_va_num = scaler.transform(X_va_num).astype(np.float32)\n",
    "X_te_num = scaler.transform(X_te_num).astype(np.float32)\n",
    "\n",
    "# ---- Categorical: mapping (train 기준) + transform\n",
    "def fit_cat_map(train_series: pd.Series):\n",
    "    s = train_series.astype(\"object\")\n",
    "    vals = s.dropna().unique().tolist()\n",
    "    vals = sorted(vals, key=lambda v: str(v))\n",
    "    mapping = {v: i + 1 for i, v in enumerate(vals)}  # 1..N\n",
    "    size = len(vals) + 1  # +1 for UNK=0\n",
    "    return mapping, size\n",
    "\n",
    "def transform_cat(df_part: pd.DataFrame, col: str, mapping: dict) -> np.ndarray:\n",
    "    s = df_part[col].astype(\"object\")\n",
    "    return s.map(mapping).fillna(0).astype(np.int64).values\n",
    "\n",
    "cat_maps = {}\n",
    "cat_sizes = {}\n",
    "X_tr_cat_list, X_va_cat_list, X_te_cat_list = [], [], []\n",
    "\n",
    "for c in CATEGORICAL_COLS:\n",
    "    m, size = fit_cat_map(X_tr[c])\n",
    "    cat_maps[c] = m\n",
    "    cat_sizes[c] = size\n",
    "\n",
    "    X_tr_cat_list.append(transform_cat(X_tr, c, m))\n",
    "    X_va_cat_list.append(transform_cat(X_va, c, m))\n",
    "    X_te_cat_list.append(transform_cat(X_te, c, m))\n",
    "\n",
    "X_tr_cat = np.stack(X_tr_cat_list, axis=1)\n",
    "X_va_cat = np.stack(X_va_cat_list, axis=1)\n",
    "X_te_cat = np.stack(X_te_cat_list, axis=1)\n",
    "\n",
    "print(\"Numeric shape:\", X_tr_num.shape)\n",
    "print(\"Cat shape    :\", X_tr_cat.shape)\n",
    "print(\"cat_sizes    :\", cat_sizes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcb49a4",
   "metadata": {},
   "source": [
    "## 5) Dataset / DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6efa9e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5) Dataset / DataLoader\n",
    "# ============================================================\n",
    "class KKBoxDataset(Dataset):\n",
    "    def __init__(self, X_num, X_cat, y):\n",
    "        self.X_num = torch.from_numpy(X_num)                  # float32\n",
    "        self.X_cat = torch.from_numpy(X_cat)                  # int64\n",
    "        self.y = torch.from_numpy(y.values.astype(np.float32))# float32\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_num[idx], self.X_cat[idx], self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(KKBoxDataset(X_tr_num, X_tr_cat, y_tr), batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(KKBoxDataset(X_va_num, X_va_cat, y_va), batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(KKBoxDataset(X_te_num, X_te_cat, y_te), batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ecd6be",
   "metadata": {},
   "source": [
    "## 6) Model (Embedding + MLP)\n",
    "\n",
    "- 출력은 `logit` 1개이며, 확률은 `sigmoid(logit)`로 변환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b611d37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbeddingMLP(\n",
      "  (emb_layers): ModuleDict(\n",
      "    (city): Embedding(22, 9)\n",
      "    (gender): Embedding(4, 4)\n",
      "    (registered_via): Embedding(6, 5)\n",
      "    (last_payment_method): Embedding(34, 12)\n",
      "    (has_ever_paid): Embedding(3, 3)\n",
      "    (has_ever_cancelled): Embedding(3, 3)\n",
      "  )\n",
      "  (mlp): Sequential(\n",
      "    (0): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): Linear(in_features=80, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.35, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.35, inplace=False)\n",
      "    (7): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (8): ReLU()\n",
      "    (9): Dropout(p=0.35, inplace=False)\n",
      "    (10): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 6) Model\n",
    "# ============================================================\n",
    "def choose_emb_dim(n_cat: int) -> int:\n",
    "    # 간단 휴리스틱: 너무 작지 않게 / 너무 크지 않게\n",
    "    return int(min(50, max(2, round(np.sqrt(n_cat) * 2))))\n",
    "\n",
    "class EmbeddingMLP(nn.Module):\n",
    "    def __init__(self, num_numeric, cat_sizes, hidden=HIDDEN, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.cat_cols = list(cat_sizes.keys())\n",
    "        self.emb_layers = nn.ModuleDict()\n",
    "\n",
    "        emb_out_dim = 0\n",
    "        for c in self.cat_cols:\n",
    "            n_cat = cat_sizes[c]\n",
    "            d = choose_emb_dim(n_cat)\n",
    "            self.emb_layers[c] = nn.Embedding(num_embeddings=n_cat, embedding_dim=d)\n",
    "            emb_out_dim += d\n",
    "\n",
    "        in_dim = num_numeric + emb_out_dim\n",
    "\n",
    "        layers = [nn.BatchNorm1d(in_dim)]\n",
    "        prev = in_dim\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(prev, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            prev = h\n",
    "        layers += [nn.Linear(prev, 1)]\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        embs = []\n",
    "        for i, c in enumerate(self.cat_cols):\n",
    "            embs.append(self.emb_layers[c](x_cat[:, i]))\n",
    "        x = torch.cat([x_num] + embs, dim=1)\n",
    "        logit = self.mlp(x).squeeze(1)\n",
    "        return logit\n",
    "\n",
    "model = EmbeddingMLP(num_numeric=X_tr_num.shape[1], cat_sizes=cat_sizes).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21bce0d",
   "metadata": {},
   "source": [
    "## 7) Train (Early Stopping by PR-AUC)\n",
    "\n",
    "- 불균형 대응: `pos_weight = #neg / #pos`\n",
    "- Early stopping 기준: **Valid PR-AUC**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63bacfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_weight: 9.570667266845703\n",
      "[Epoch 01] train_loss=0.61835 | valid PR_AUC=0.78297 ROC_AUC=0.94116 LogLoss=0.29753 Acc@0.5=0.89608\n",
      "[Epoch 02] train_loss=0.55407 | valid PR_AUC=0.78772 ROC_AUC=0.94173 LogLoss=0.28619 Acc@0.5=0.89823\n",
      "[Epoch 03] train_loss=0.54934 | valid PR_AUC=0.78866 ROC_AUC=0.94280 LogLoss=0.26818 Acc@0.5=0.89982\n",
      "[Epoch 04] train_loss=0.54626 | valid PR_AUC=0.78978 ROC_AUC=0.94303 LogLoss=0.26548 Acc@0.5=0.90129\n",
      "[Epoch 05] train_loss=0.54513 | valid PR_AUC=0.79060 ROC_AUC=0.94319 LogLoss=0.29138 Acc@0.5=0.89854\n",
      "[Epoch 06] train_loss=0.54212 | valid PR_AUC=0.79025 ROC_AUC=0.94336 LogLoss=0.29700 Acc@0.5=0.89184\n",
      "[Epoch 07] train_loss=0.54256 | valid PR_AUC=0.79114 ROC_AUC=0.94326 LogLoss=0.28076 Acc@0.5=0.89496\n",
      "[Epoch 08] train_loss=0.54025 | valid PR_AUC=0.79163 ROC_AUC=0.94370 LogLoss=0.27417 Acc@0.5=0.89502\n",
      "[Epoch 09] train_loss=0.54202 | valid PR_AUC=0.79211 ROC_AUC=0.94378 LogLoss=0.27840 Acc@0.5=0.89681\n",
      "[Epoch 10] train_loss=0.54053 | valid PR_AUC=0.79179 ROC_AUC=0.94329 LogLoss=0.28816 Acc@0.5=0.89074\n",
      "[Epoch 11] train_loss=0.53912 | valid PR_AUC=0.79265 ROC_AUC=0.94382 LogLoss=0.26460 Acc@0.5=0.89856\n",
      "[Epoch 12] train_loss=0.53821 | valid PR_AUC=0.79227 ROC_AUC=0.94391 LogLoss=0.26271 Acc@0.5=0.89820\n",
      "[Epoch 13] train_loss=0.53835 | valid PR_AUC=0.79234 ROC_AUC=0.94399 LogLoss=0.27360 Acc@0.5=0.89885\n",
      "[Epoch 14] train_loss=0.53805 | valid PR_AUC=0.79257 ROC_AUC=0.94392 LogLoss=0.27284 Acc@0.5=0.89517\n",
      "[Epoch 15] train_loss=0.53723 | valid PR_AUC=0.79199 ROC_AUC=0.94380 LogLoss=0.27674 Acc@0.5=0.89760\n",
      "[Epoch 16] train_loss=0.53802 | valid PR_AUC=0.79274 ROC_AUC=0.94402 LogLoss=0.28178 Acc@0.5=0.89602\n",
      "[Epoch 17] train_loss=0.53695 | valid PR_AUC=0.79306 ROC_AUC=0.94381 LogLoss=0.27726 Acc@0.5=0.89441\n",
      "[Epoch 18] train_loss=0.53695 | valid PR_AUC=0.79217 ROC_AUC=0.94368 LogLoss=0.26862 Acc@0.5=0.89797\n",
      "[Epoch 19] train_loss=0.53578 | valid PR_AUC=0.79295 ROC_AUC=0.94404 LogLoss=0.28566 Acc@0.5=0.89638\n",
      "[Epoch 20] train_loss=0.53599 | valid PR_AUC=0.79328 ROC_AUC=0.94396 LogLoss=0.28460 Acc@0.5=0.89299\n",
      "[Epoch 21] train_loss=0.53393 | valid PR_AUC=0.79323 ROC_AUC=0.94404 LogLoss=0.26612 Acc@0.5=0.89928\n",
      "[Epoch 22] train_loss=0.53587 | valid PR_AUC=0.79318 ROC_AUC=0.94390 LogLoss=0.29832 Acc@0.5=0.88988\n",
      "[Epoch 23] train_loss=0.53368 | valid PR_AUC=0.79328 ROC_AUC=0.94392 LogLoss=0.28184 Acc@0.5=0.89766\n",
      "[Epoch 24] train_loss=0.53466 | valid PR_AUC=0.79264 ROC_AUC=0.94388 LogLoss=0.27192 Acc@0.5=0.89557\n",
      "[Epoch 25] train_loss=0.53324 | valid PR_AUC=0.79291 ROC_AUC=0.94400 LogLoss=0.27415 Acc@0.5=0.90030\n",
      "Early stopping. Best PR_AUC=0.79328\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 7) Train\n",
    "# ============================================================\n",
    "n_pos = int((y_tr.values == 1).sum())\n",
    "n_neg = int((y_tr.values == 0).sum())\n",
    "pos_weight = torch.tensor([n_neg / max(1, n_pos)], dtype=torch.float32, device=device)\n",
    "print(\"pos_weight:\", float(pos_weight.item()))\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_proba(loader):\n",
    "    model.eval()\n",
    "    probs, ys = [], []\n",
    "    for x_num, x_cat, yb in loader:\n",
    "        x_num = x_num.to(device)\n",
    "        x_cat = x_cat.to(device)\n",
    "\n",
    "        logit = model(x_num, x_cat)\n",
    "        p = torch.sigmoid(logit).detach().cpu().numpy()\n",
    "        probs.append(p)\n",
    "        ys.append(yb.numpy())\n",
    "    return np.concatenate(ys), np.concatenate(probs)\n",
    "\n",
    "def eval_core_metrics(y_true, y_prob, threshold=0.5):\n",
    "    y_prob = np.clip(y_prob, 1e-7, 1 - 1e-7)\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    return {\n",
    "        \"PR_AUC\": average_precision_score(y_true, y_prob),\n",
    "        \"ROC_AUC\": roc_auc_score(y_true, y_prob),\n",
    "        \"LogLoss\": log_loss(y_true, y_prob),\n",
    "        \"Accuracy@0.5\": accuracy_score(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "best_pr = -1.0\n",
    "best_state = None\n",
    "pat_cnt = 0\n",
    "\n",
    "for epoch in range(1, MAX_EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss, n_batches = 0.0, 0\n",
    "\n",
    "    for x_num, x_cat, yb in train_loader:\n",
    "        x_num = x_num.to(device)\n",
    "        x_cat = x_cat.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(x_num, x_cat)\n",
    "        loss = criterion(logit, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "        n_batches += 1\n",
    "\n",
    "    # valid\n",
    "    yv, pv = predict_proba(valid_loader)\n",
    "    m = eval_core_metrics(yv, pv, threshold=0.5)\n",
    "    train_loss = total_loss / max(1, n_batches)\n",
    "\n",
    "    print(f\"[Epoch {epoch:02d}] train_loss={train_loss:.5f} | \"\n",
    "          f\"valid PR_AUC={m['PR_AUC']:.5f} ROC_AUC={m['ROC_AUC']:.5f} \"\n",
    "          f\"LogLoss={m['LogLoss']:.5f} Acc@0.5={m['Accuracy@0.5']:.5f}\")\n",
    "\n",
    "    if m[\"PR_AUC\"] > best_pr + 1e-5:\n",
    "        best_pr = m[\"PR_AUC\"]\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        pat_cnt = 0\n",
    "    else:\n",
    "        pat_cnt += 1\n",
    "        if pat_cnt >= PATIENCE:\n",
    "            print(f\"Early stopping. Best PR_AUC={best_pr:.5f}\")\n",
    "            break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0307bf",
   "metadata": {},
   "source": [
    "## 8) Test Evaluation + Gap (Valid ↔ Test)\n",
    "\n",
    "- Valid/Test 성능을 모두 기록하고, gap을 수치로 계산합니다.\n",
    "- Confusion Matrix / Classification Report는 threshold=0.5 기준(필요 시 추후 조정).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e0c1b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[VALID METRICS]\n",
      "PR_AUC      : 0.793281\n",
      "ROC_AUC     : 0.943961\n",
      "LogLoss     : 0.284601\n",
      "Accuracy@0.5: 0.892989\n",
      "\n",
      "[TEST METRICS]\n",
      "PR_AUC      : 0.788933\n",
      "ROC_AUC     : 0.942006\n",
      "LogLoss     : 0.285698\n",
      "Accuracy@0.5: 0.890929\n",
      "\n",
      "[GAP (TEST - VALID)]\n",
      "gap_PR_AUC  : -0.004348\n",
      "gap_ROC_AUC : -0.001955\n",
      "gap_LogLoss : 0.001098\n",
      "gap_Accuracy@0.5: -0.002060\n",
      "\n",
      "[Confusion Matrix @0.5]\n",
      "[[104688  12240]\n",
      " [  1846  10371]]\n",
      "\n",
      "[Classification Report @0.5]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9827    0.8953    0.9370    116928\n",
      "         1.0     0.4587    0.8489    0.5956     12217\n",
      "\n",
      "    accuracy                         0.8909    129145\n",
      "   macro avg     0.7207    0.8721    0.7663    129145\n",
      "weighted avg     0.9331    0.8909    0.9047    129145\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8) Eval (Valid/Test + gap) + Confusion/Report\n",
    "# ============================================================\n",
    "yv, pv = predict_proba(valid_loader)\n",
    "yt, pt = predict_proba(test_loader)\n",
    "\n",
    "mv = eval_core_metrics(yv, pv, threshold=0.5)\n",
    "mt = eval_core_metrics(yt, pt, threshold=0.5)\n",
    "\n",
    "gap = {f\"gap_{k}\": (mt[k] - mv[k]) for k in mv.keys()}\n",
    "\n",
    "print(\"\\n[VALID METRICS]\")\n",
    "for k, v in mv.items():\n",
    "    print(f\"{k:12s}: {v:.6f}\")\n",
    "\n",
    "print(\"\\n[TEST METRICS]\")\n",
    "for k, v in mt.items():\n",
    "    print(f\"{k:12s}: {v:.6f}\")\n",
    "\n",
    "print(\"\\n[GAP (TEST - VALID)]\")\n",
    "for k, v in gap.items():\n",
    "    print(f\"{k:12s}: {v:.6f}\")\n",
    "\n",
    "# Confusion Matrix + Classification Report (threshold=0.5)\n",
    "y_pred = (pt >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n[Confusion Matrix @0.5]\")\n",
    "print(confusion_matrix(yt, y_pred))\n",
    "\n",
    "print(\"\\n[Classification Report @0.5]\")\n",
    "print(classification_report(yt, y_pred, digits=4, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233a32c4",
   "metadata": {},
   "source": [
    "## 9) (옵션) Permutation Importance (PR-AUC drop)\n",
    "\n",
    "- 딥러닝은 트리 모델처럼 `feature_importances_`가 기본 제공되지 않아,\n",
    "  **Permutation Importance**로 “해당 피처를 섞었을 때 PR-AUC가 얼마나 떨어지는지(drop)”를 계산합니다.\n",
    "- drop이 클수록 모델이 해당 피처에 더 의존합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97eb5890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Permutation Importance] Top (by PR-AUC drop)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>group</th>\n",
       "      <th>base_pr_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>drop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>has_ever_cancelled</td>\n",
       "      <td>CAT</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.354401</td>\n",
       "      <td>0.434532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>has_ever_paid</td>\n",
       "      <td>CAT</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.432195</td>\n",
       "      <td>0.356738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>last_payment_method</td>\n",
       "      <td>CAT</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.645790</td>\n",
       "      <td>0.143143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reg_days</td>\n",
       "      <td>NUMERIC</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.769726</td>\n",
       "      <td>0.019207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>num_days_active_w7</td>\n",
       "      <td>NUMERIC</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.779369</td>\n",
       "      <td>0.009564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>registered_via</td>\n",
       "      <td>CAT</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.783266</td>\n",
       "      <td>0.005667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>city</td>\n",
       "      <td>CAT</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.784257</td>\n",
       "      <td>0.004676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>completion_ratio_w21</td>\n",
       "      <td>NUMERIC</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.786745</td>\n",
       "      <td>0.002188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>num_days_active_w14</td>\n",
       "      <td>NUMERIC</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.787428</td>\n",
       "      <td>0.001505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gender</td>\n",
       "      <td>CAT</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.787707</td>\n",
       "      <td>0.001226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>num_days_active_w21</td>\n",
       "      <td>NUMERIC</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.788022</td>\n",
       "      <td>0.000911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>short_play_ratio_w21</td>\n",
       "      <td>NUMERIC</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.788117</td>\n",
       "      <td>0.000816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>completion_ratio_w7</td>\n",
       "      <td>NUMERIC</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.788200</td>\n",
       "      <td>0.000733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>completion_ratio_w14</td>\n",
       "      <td>NUMERIC</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.788252</td>\n",
       "      <td>0.000681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>skip_ratio_w7</td>\n",
       "      <td>NUMERIC</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.788394</td>\n",
       "      <td>0.000539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>num_unq_w21</td>\n",
       "      <td>NUMERIC</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.788440</td>\n",
       "      <td>0.000493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>num_songs_w21</td>\n",
       "      <td>NUMERIC</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.788477</td>\n",
       "      <td>0.000456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>days_trend_w7_w14</td>\n",
       "      <td>NUMERIC</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.788494</td>\n",
       "      <td>0.000439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>variety_ratio_w21</td>\n",
       "      <td>NUMERIC</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.788569</td>\n",
       "      <td>0.000364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>std_secs_w14</td>\n",
       "      <td>NUMERIC</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.788640</td>\n",
       "      <td>0.000293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>avg_secs_per_day_w21</td>\n",
       "      <td>NUMERIC</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.788707</td>\n",
       "      <td>0.000226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>variety_ratio_w14</td>\n",
       "      <td>NUMERIC</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.788707</td>\n",
       "      <td>0.000226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>short_play_ratio_w7</td>\n",
       "      <td>NUMERIC</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.788708</td>\n",
       "      <td>0.000225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>variety_ratio_w7</td>\n",
       "      <td>NUMERIC</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.788721</td>\n",
       "      <td>0.000212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>short_play_w21</td>\n",
       "      <td>NUMERIC</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.788749</td>\n",
       "      <td>0.000184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>avg_songs_per_day_w14</td>\n",
       "      <td>NUMERIC</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.788762</td>\n",
       "      <td>0.000171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>avg_secs_per_day_w14</td>\n",
       "      <td>NUMERIC</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.788796</td>\n",
       "      <td>0.000137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>num_100_w21</td>\n",
       "      <td>NUMERIC</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.788812</td>\n",
       "      <td>0.000121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>avg_songs_per_day_w7</td>\n",
       "      <td>NUMERIC</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.788813</td>\n",
       "      <td>0.000120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>avg_secs_per_day_w7</td>\n",
       "      <td>NUMERIC</td>\n",
       "      <td>0.788933</td>\n",
       "      <td>0.788821</td>\n",
       "      <td>0.000113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  feature    group  base_pr_auc    pr_auc      drop\n",
       "0      has_ever_cancelled      CAT     0.788933  0.354401  0.434532\n",
       "1           has_ever_paid      CAT     0.788933  0.432195  0.356738\n",
       "2     last_payment_method      CAT     0.788933  0.645790  0.143143\n",
       "3                reg_days  NUMERIC     0.788933  0.769726  0.019207\n",
       "4      num_days_active_w7  NUMERIC     0.788933  0.779369  0.009564\n",
       "5          registered_via      CAT     0.788933  0.783266  0.005667\n",
       "6                    city      CAT     0.788933  0.784257  0.004676\n",
       "7    completion_ratio_w21  NUMERIC     0.788933  0.786745  0.002188\n",
       "8     num_days_active_w14  NUMERIC     0.788933  0.787428  0.001505\n",
       "9                  gender      CAT     0.788933  0.787707  0.001226\n",
       "10    num_days_active_w21  NUMERIC     0.788933  0.788022  0.000911\n",
       "11   short_play_ratio_w21  NUMERIC     0.788933  0.788117  0.000816\n",
       "12    completion_ratio_w7  NUMERIC     0.788933  0.788200  0.000733\n",
       "13   completion_ratio_w14  NUMERIC     0.788933  0.788252  0.000681\n",
       "14          skip_ratio_w7  NUMERIC     0.788933  0.788394  0.000539\n",
       "15            num_unq_w21  NUMERIC     0.788933  0.788440  0.000493\n",
       "16          num_songs_w21  NUMERIC     0.788933  0.788477  0.000456\n",
       "17      days_trend_w7_w14  NUMERIC     0.788933  0.788494  0.000439\n",
       "18      variety_ratio_w21  NUMERIC     0.788933  0.788569  0.000364\n",
       "19           std_secs_w14  NUMERIC     0.788933  0.788640  0.000293\n",
       "20   avg_secs_per_day_w21  NUMERIC     0.788933  0.788707  0.000226\n",
       "21      variety_ratio_w14  NUMERIC     0.788933  0.788707  0.000226\n",
       "22    short_play_ratio_w7  NUMERIC     0.788933  0.788708  0.000225\n",
       "23       variety_ratio_w7  NUMERIC     0.788933  0.788721  0.000212\n",
       "24         short_play_w21  NUMERIC     0.788933  0.788749  0.000184\n",
       "25  avg_songs_per_day_w14  NUMERIC     0.788933  0.788762  0.000171\n",
       "26   avg_secs_per_day_w14  NUMERIC     0.788933  0.788796  0.000137\n",
       "27            num_100_w21  NUMERIC     0.788933  0.788812  0.000121\n",
       "28   avg_songs_per_day_w7  NUMERIC     0.788933  0.788813  0.000120\n",
       "29    avg_secs_per_day_w7  NUMERIC     0.788933  0.788821  0.000113"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 9) Permutation Importance (Optional)\n",
    "# ============================================================\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def batched_predict_proba_from_arrays(X_num, X_cat, batch_size=8192):\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    n = X_num.shape[0]\n",
    "    for i in range(0, n, batch_size):\n",
    "        xb_num = torch.from_numpy(X_num[i:i+batch_size]).to(device)\n",
    "        xb_cat = torch.from_numpy(X_cat[i:i+batch_size]).to(device)\n",
    "        logit = model(xb_num, xb_cat)\n",
    "        p = torch.sigmoid(logit).detach().cpu().numpy()\n",
    "        probs.append(p)\n",
    "    return np.concatenate(probs)\n",
    "\n",
    "def permutation_importance_pr_auc(\n",
    "    y_true, X_num, X_cat,\n",
    "    numeric_cols, categorical_cols,\n",
    "    base_prob=None,\n",
    "    top_n=30,\n",
    "    random_state=RANDOM_STATE,\n",
    "):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    if base_prob is None:\n",
    "        base_prob = batched_predict_proba_from_arrays(X_num, X_cat)\n",
    "    base_pr = average_precision_score(y_true, base_prob)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # numeric\n",
    "    for j, fname in enumerate(numeric_cols):\n",
    "        X_num_p = X_num.copy()\n",
    "        X_num_p[:, j] = rng.permutation(X_num_p[:, j])\n",
    "        p = batched_predict_proba_from_arrays(X_num_p, X_cat)\n",
    "        pr = average_precision_score(y_true, p)\n",
    "        rows.append({\"feature\": fname, \"group\": \"NUMERIC\", \"base_pr_auc\": base_pr, \"pr_auc\": pr, \"drop\": base_pr - pr})\n",
    "\n",
    "    # categorical (embedding index)\n",
    "    for j, fname in enumerate(categorical_cols):\n",
    "        X_cat_p = X_cat.copy()\n",
    "        X_cat_p[:, j] = rng.permutation(X_cat_p[:, j])\n",
    "        p = batched_predict_proba_from_arrays(X_num, X_cat_p)\n",
    "        pr = average_precision_score(y_true, p)\n",
    "        rows.append({\"feature\": fname, \"group\": \"CAT\", \"base_pr_auc\": base_pr, \"pr_auc\": pr, \"drop\": base_pr - pr})\n",
    "\n",
    "    imp_df = pd.DataFrame(rows).sort_values(\"drop\", ascending=False).reset_index(drop=True)\n",
    "    return imp_df.head(top_n)\n",
    "\n",
    "if RUN_PERM_IMPORTANCE:\n",
    "    # 샘플 수 제한(속도)\n",
    "    n = X_te_num.shape[0]\n",
    "    use_n = min(n, PERM_MAX_SAMPLES)\n",
    "    Xn = X_te_num[:use_n]\n",
    "    Xc = X_te_cat[:use_n]\n",
    "    yy = y_te.values[:use_n]\n",
    "\n",
    "    base_prob = batched_predict_proba_from_arrays(Xn, Xc)\n",
    "    imp_df = permutation_importance_pr_auc(\n",
    "        y_true=yy, X_num=Xn, X_cat=Xc,\n",
    "        numeric_cols=NUMERICAL_COLS,\n",
    "        categorical_cols=CATEGORICAL_COLS,\n",
    "        base_prob=base_prob,\n",
    "        top_n=PERM_TOP_N,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "    print(\"\\n[Permutation Importance] Top (by PR-AUC drop)\")\n",
    "    display(imp_df)\n",
    "else:\n",
    "    print(\"RUN_PERM_IMPORTANCE=False (skipped)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_basic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
